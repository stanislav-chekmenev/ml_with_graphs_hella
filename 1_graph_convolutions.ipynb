{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8gXnV_XN3JN8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKYUq1YdN_vK",
    "tags": []
   },
   "source": [
    "# **Graph convolutions**\n",
    "\n",
    "*   GCNs are similar to convolutions on images in the sense that the weights are typically shared over all locations in the graph.\n",
    "*   GCNs rely on message passing methods, which means that vertices exchange information with the neighbours.\n",
    "\n",
    "### **Message passing**\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"> <img src =\"pictures/message_passing.png\" width=\"700px\"></center>\n",
    "\n",
    "\n",
    "*   Step 1: Each node creates a \"*message*\", i.e. a feature vector.\n",
    "*   Step 2: The messages are sent to the neighbours, so a node receives one message from a neighbour if they are connected via an edge.\n",
    "*   Step 3: An aggregation function is applied to the messages received by each node.\n",
    "    - Typical aggregation functions: *sum*, *mean*.\n",
    "\n",
    "Let's define the message passing in mathematical terms:\n",
    "\n",
    "> * $x_i$ - a feature vector of node $i$ summarized in an $N \\times D$ matrix $X$, where $N$ is the number of nodes, $D$ is the number of input features.\n",
    "* $\\hat A = A + I$ - sum of the adjacency matrix and an identity matrix. $\\hat A \\in R^{N \\times N}$ This way we have edges corresponding to the connection of a node to itself.\n",
    "* $\\hat D$ - a diagonal node degree matrix of $\\hat A$.\n",
    "* $H^l$ - a feature matrix of the $l$-th layer. $H^0 = X$\n",
    "* $W^l$ - a weight matrix of the $l$-th layer.\n",
    "* $\\sigma$ - activation function, typically of the ReLU family.\n",
    "\n",
    "$$H^{l+1} = \\sigma\\left(\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}H^{l}W^{l}\\right)$$\n",
    "\n",
    "**$H^l W^l$ creates a message made of node features and multiplication with $\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}$ is responsible for normalization and averaging of the messages that arrived from the neighbours.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtoGQIj-Fkg9"
   },
   "source": [
    "### **Graph convolutional layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VCBRi-zRNwMV"
   },
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(c_in, c_out)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
    "                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections. \n",
    "                         Shape: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        # TODO: write the forward pass together.\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkMFBeOV-O1m"
   },
   "source": [
    "### **Exercise 1.1: Understanding the GCN layer**\n",
    "\n",
    "To further understand the GCN layer, apply it to the example graph above. \n",
    "\n",
    "Using `torch`, please, do the following:\n",
    "\n",
    "*   Create the adjacency matrix for the example graph above.\n",
    "      - **hint**: use `torch.Tensor`.\n",
    "*   Create a feature tensor `node_feats` of the shape (1, 4, 2), aranging integer numbers from 0 to 8. \n",
    "      - **hint**: use `torch.float32` dtype.\n",
    "      - **hint**: check `torch.arange` and `torch.view`. \n",
    "*   Create a `gcn_layer`, using the class we wrote above and intialize the linear weight projection matrix as an identity matrix and set the biases to zero, so you could easily see the message passing mechanism in action, since the input features will be equal to the messages in this case.\n",
    "      - **hint**: set `c_in` and `c_out` in such a way that the node features will have the same shape after the `gcn_layer` has been applied.\n",
    "      - **hint**: change these two attributes: `self.projection.weight.data` and `self.projection.bias.data`\n",
    "*   Create an output feature tensor `out_feats` by passing it through the GCN layer (don't forget to use the adjacency matrix in the forward pass).\n",
    "*   Print: the adjacency matrix, `node_feats` and `out_feats`.\n",
    "*   Verify the outputs of the nodes. Are they the averages of the neighbouring node features? Can you prove it?\n",
    "*   Think about the questions: \n",
    "      - How can we pass the information between the nodes 1 and 3?\n",
    "      - How does the computation time for receiveing a message for node $i$ scale when we increase the number of nodes that we want to include in the message passing procedure? Assume that the path between $i$ and those nodes exists.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SOkTJ1w_LaME"
   },
   "outputs": [],
   "source": [
    "# TODO: Complete the exercise 1.1 here.\n",
    "# ... (~ 15 lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-fX8QKXiOMA"
   },
   "source": [
    "### **GCN and its limitation**\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"> <img src =\"pictures/gcn_web.png\" width=\"600px\"></center>\n",
    "\n",
    "\n",
    "**The features for nodes 3 and 4 are the same because they have the same adjacent nodes (including itself). Therefore, GCN layers can make the network forget node-specific information if we just take a mean over all messages.** (figure credit - [Thomas Kipf, 2016](https://tkipf.github.io/graph-convolutional-networks/))\n",
    "\n",
    "#### Solutions:\n",
    "*   Residual connections.\n",
    "*   Weigh the self-connections higher.\n",
    "*   Define a separate weight matrix for the self-connections. \n",
    "*   **Compute weights dynamically using graph attention.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5H4xGGNv3qw"
   },
   "source": [
    "### **Graph attention networks**\n",
    "\n",
    "\n",
    "*   Attention describes a weighted average of multiple elements with the weights dynamically computed based on an input query and elements' keys (figure credit - [Tutorial on transformers and attention mechanism](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)).\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"> <img src =\"pictures/attention_example.png\" width=\"700px\"></center>\n",
    "\n",
    "*   As in the GCN, the graph attention (GAT) layer creates a message for each node using a weight matrix.\n",
    "*   **Query** - the message from the node itself.\n",
    "*   **Keys = Values** - the messages from the neighbours and **the node itself**.\n",
    "*   **The score function** - a one-layer MLP with the LeakyReLU non-linearity.\n",
    "*   The graph structure is injected via masking some of the nodes when attention coefficients are computed. Usually one takes into account only the first-order neighbours of a node $N_i$.\n",
    "\n",
    "$$\\alpha_{ij} = \\frac{\\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_j\\right]\\right)\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_k\\right]\\right)\\right)}$$\n",
    "\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"> <img src =\"pictures/gat_mechanism.png\" width=\"300px\"></center>\n",
    "\n",
    "The final message of the $i$-th node is computed according to:\n",
    "\n",
    "$$\\mathbf{h_i}'=\\sigma\\left(\\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}\\mathbf{W}\\mathbf{h_j}\\right)$$\n",
    "\n",
    "#### **Multi-head attention**\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"> <img src =\"pictures/multihead_attention.jpeg\" width=\"400px\"></center>\n",
    "\n",
    "\n",
    "*   For layers from $0$ to $L-1$:\n",
    "  $$\\mathbf{h_i}'= \\parallel_{k=1}^{K} \\sigma\\left(\\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}^k\\mathbf{W}^k\\mathbf{h_j}\\right)$$\n",
    "*   For the last layer L:\n",
    "$$\\mathbf{h_i}'= \\sigma\\left(  \\frac{1}{K} \\sum_{k=1}^{K} \\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}^k\\mathbf{W}^k\\mathbf{h_j}\\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNUOD7hryZWdEfb588e7l1a",
   "collapsed_sections": [],
   "name": "1_graph_convolutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
